{
 "metadata": {
  "name": "",
  "signature": "sha256:2341936f1148d2456a2309f49a0354941a2f1e34c2f85c17ae189065f04efecf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Mathematics examples applied to fluid mechanics\n",
      "\n",
      "Recall that the use of GPUs to accelerate applications is best when the problem at hand can be parallelized, this means that the same operations on similar data structures are repeated during the solution. In this section we develop some examples taken mostly from fluid mechanics, to show how the same operations are repeated on the data and how they can be parallelized to obtain a better efficiency.\n",
      "\n",
      "The problems we will work in this chapter are a numerical solution to the\n",
      "\n",
      "1. Two dimensional diffusion equation;\n",
      "2. Two dimensional wave equation using a spectral methods;\n",
      "3. Solution to linear systems.\n",
      "\n",
      "These problems are chosen because 1) the mathematical concepts are not too difficult, 2) their range of applications in different disciplines and most importantly for our purposes, 3) being both computationally intensive and massively parallel, they are good candidates for parallelization.\n",
      "\n",
      "By _computationally intensive_ we mean **that the time spent on computation significantly exceeds the time spent on transferring data to and from GPU memory** - this is important because the bottleneck of any GPU application is data transfer, while by _massively parallel_ we mean taht **the computations can be broken down into hundreds or thousands of independent units of work**\n",
      "\n",
      "The details of each problem are described elsewhere. Here we briefly state the problem and implement the solution. (Three) Different approaches to the solutions will be described: the first one will use the closest to \"pure Python\" as possible; the second apporach will use PyCUDA and the last approaches will build on the Python code to create computer code which runs on the CPU and the GPU withouth needing to go through the details of the CUDA programming model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. Two dimensional diffusion equation\n",
      "The 2D diffusion equation is\n",
      "\n",
      "$$\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2} + \\alpha \\frac{\\partial^2 u}{\\partial y^2}$$\n",
      "\n",
      "where $u$ represents a scalar field, for example temperature. This second order differential equation must be accompanied by boundary and initial conditions. We will solve this equation for a square region of size $L$ which has two of its sides at a constant temperature, subject to the b.c. that the temperature on these two sides must remain constant. The figure below shows this initial condition with the Dirichlet boundary conditions.\n",
      "![caption](files/sol_0.png)\n",
      "\n",
      "We will use the value $\\alpha = 0.645$ for the diffusion coefficient. The differential equation can be discretized using a finite differences method as:\n",
      "\n",
      "$$\\frac{u^{n+1}_{i,j}-u^n_{i,j}}{\\Delta t} = \\alpha \\frac{u^n_{i+1,j}-2u^n_{i,j}+u^n_{i-1,j}}{\\Delta x^2} + \\alpha \\frac{u^n_{i,j+1}-2u^n_{i,j}+u^n_{i,j-1}}{\\Delta y^2}$$\n",
      "\n",
      "which can be solved for $u^{n+1}_{i,j}$: \n",
      "\n",
      "$$u^{n+1}_{i,j} = u^n_{i,j} + \\frac{\\alpha \\Delta t}{\\Delta x^2}(u^n_{i+1,j}-2u^n_{i,j}+u^n_{i-1,j}) + \\frac{\\alpha \\Delta t}{\\Delta y^2}(u^n_{i,j+1}-2u^n_{i,j}+u^n_{i,j-1})$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pure Python\n",
      "We first set the problem's conditions and solve using Python. The following code imports some libraries needed to work out the solution and plots the initial condition shown above.\n",
      "\n",
      "Now we are ready to solve the diffusion equation using Python code. The most efficient way to do so is to write a function that performs the operations in one call. The code to this function is in the cell below, and we have named it `diffuse_cpu(nx, ny, bool_plot)`, the first two arguments indicate the grid size for the region, while the last argument determines if the result is plotted.\n",
      "\n",
      "The results obtained with this numerical approach for square region divided into a 64x64 are shown below for different times.\n",
      "![Solution for $t=0$](files/sol_0.png) \n",
      "![Solution for $t=10$](files/sol_10.png) \n",
      "![Solution for $t=100$](files/sol_100.png)\n",
      "\n",
      "Notice how as time advances the distribution of heat caused by the initial temperature on the sides (the boundary conditions) diffuses throught the region. Later we will time the execution of this function to compare with the performance of the GPU."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PyCUDA\n",
      "Let's now approach the solution using the GPU by means of PyCUDA. PyCUDA is a Python library that offers bindings to the CUDA API from Python. The complete code is wrapped in a function called `diffuse_gpu(nx, ny, bool)` and takes the same arguments as its CPU counterpart.\n",
      "\n",
      "The use of PyCUDA does not abstract away the CUDA programming model, and we still need to write the kernels that will be loaded into the GPU. These are created using instances of PyCUDA's `SourceModule` (see the code below). After the kernels are loaded into the GPU and waiting for instructions to execute, they can be called directly from Python\n",
      "\n",
      "The remaining part of the implementation follows the CUDA programming model closely:\n",
      "1. Initialize data arrays in the GPU;\n",
      "2. Copy dat a to GPU;\n",
      "3. Execute kernels;\n",
      "4. Copy data back from GPU to host.\n",
      "\n",
      "PyCUDA contains the corresponding classes to perform this operations transparently, avoiding details of the CUDA keywords. It is convenient to take advantage of Pythons array manipulation to create and pass data to the GPU. We do so using NumPy capabilites, taking notice that at the time, the GPU only supports 32 bits, while NumPy creates by default 64 bit data.\n",
      "\n",
      "Of course the final solution is exactly the same as the obtained using the Python code above. The differences are quantified by finding the solution for a larged grid. We compare the solutions obtained by using the pure Python and PyCUDA codes for a fixed time interval of `t=0.4` for different sizes the grid used to obtain the solution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for gs in 2**np.arange(6,10):\n",
      "    print 'grid %sx%s'%(gs,gs)\n",
      "    %timeit diffuse_python(gs, gs, 0)\n",
      "    %timeit diffuse_pycuda(gs, gs, 0)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grid 64x64\n",
        "1 loops, best of 3: 4.24 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1 loops, best of 3: 1.81 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "grid 128x128\n",
        "1 loops, best of 3: 13 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1 loops, best of 3: 1.81 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "grid 256x256\n",
        "1 loops, best of 3: 31.3 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1 loops, best of 3: 1.89 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "grid 512x512\n",
        "1 loops, best of 3: 2min 29s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1 loops, best of 3: 4.88 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing we observe is that there is a big difference in the execution times between the CPU and the GPU. Second, we observe that the CPU times depend strongly on the size of the grid, while the GPU only starts to depend on the grid sizes for large values of the grid. This investigation is a good test for Omar's thesis work, as it is to understand what happens when we change the size of the grid block and the number of threads:\n",
      "    \n",
      "    BLOCKSIZE = 16\n",
      "    gridSize = (int(ceil(nx/BLOCKSIZE)),int(ceil(nx/BLOCKSIZE)),1)\n",
      "    blockSize = (BLOCKSIZE,BLOCKSIZE,1)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Numba jit & autojit\n",
      "From the _Continuum_ site: \"Numba is an just-in-time specializing compiler which compiles annotated Python and NumPy code to LLVM (through decorators). Its goal is to seamlessly integrate with the Python scientific software stack and produce optimized native code, as well as integrate with native foreign languages\".\n",
      "\n",
      "To use Numba we just precede the Python code with @jit (after importing the numba library):\n",
      "\n",
      "    from numba import jit\n",
      "    @jit\n",
      "    def diffuse_numba(nx,ny, bool_save):\n",
      "    ...\n",
      "The remaining code for `diffuse_numba` is exactly the same as the code for `diffuse_python`. Next we compare execution times. Let's first compare between Python (only CPU):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from diffuse import *\n",
      "for gs in 2**np.arange(6,10):\n",
      "    print 'Grid Size: %sx%s'%(gs,gs)\n",
      "    print 'Python:'\n",
      "    u = np.zeros((gs,gs))\n",
      "    %timeit diffuse_python(u)\n",
      "    print 'Python jit:'\n",
      "    u = np.zeros((gs,gs))\n",
      "    %timeit fast_diffuse(u)\n",
      "    print 'Python autojit:'\n",
      "    u = np.zeros((gs,gs))\n",
      "    %timeit flex_diffuse(u)\n",
      "    print 'PyCUDA:'\n",
      "    u = np.zeros((gs,gs))\n",
      "    %timeit diffuse_pycuda(u)\n",
      "    print 'PyCUDA jit:'\n",
      "    u = np.zeros((gs,gs))\n",
      "    %timeit super_fast_diffuse(u)\n",
      "    print 'PyCUDA autojit:'\n",
      "    u = np.zeros((gs,gs))\n",
      "    %timeit super_flex_diffuse(u)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grid 64x64\n",
        "\n",
        "Python:\n",
        "1 loops, best of 3: 3.95 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Python jit:\n",
        "1 loops, best of 3: 4.74 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pytho nautojit:\n",
        "1 loops, best of 3: 4.72 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda:\n",
        "1 loops, best of 3: 1.81 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda autojit:\n",
        "1 loops, best of 3: 1.89 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "grid 128x128\n",
        "\n",
        "Python:\n",
        "1 loops, best of 3: 9.47 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Python jit:\n",
        "1 loops, best of 3: 10.1 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pytho nautojit:\n",
        "1 loops, best of 3: 10 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda:\n",
        "1 loops, best of 3: 1.8 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda autojit:\n",
        "1 loops, best of 3: 1.91 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "grid 256x256\n",
        "\n",
        "Python:\n",
        "1 loops, best of 3: 27.9 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Python jit:\n",
        "1 loops, best of 3: 35.8 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pytho nautojit:\n",
        "1 loops, best of 3: 33.6 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda:\n",
        "1 loops, best of 3: 1.78 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda autojit:\n",
        "1 loops, best of 3: 1.88 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "grid 512x512\n",
        "\n",
        "Python:\n",
        "1 loops, best of 3: 3min 3s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Python jit:\n",
        "1 loops, best of 3: 3min 9s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pytho nautojit:\n",
        "1 loops, best of 3: 3min 17s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda:\n",
        "1 loops, best of 3: 4.79 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pycuda autojit:\n",
        "1 loops, best of 3: 4.79 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file diffuse.py\n",
      "\n",
      "import numpy as np\n",
      "from numba import double, jit, autojit\n",
      "\n",
      "\n",
      "# python\n",
      "def diffuse_python(u):\n",
      "    \n",
      "    nx,ny = u.shape\n",
      "    alpha = 0.645\n",
      "    dx = 3.5/(nx-1)\n",
      "    dy = 3.5/(ny-1)\n",
      "#     sigma = 0.25\n",
      "#     dt = sigma*dx*dx/alpha\n",
      "    dt = np.float64(1e-05)\n",
      "    time = np.float64(0.4)\n",
      "    nt = np.int64(np.ceil(time/dt))\n",
      "#     print nt\n",
      "\n",
      "#     u = np.zeros((ny,nx))\n",
      "    for n in range(nt+1): \n",
      "        un = u.copy()\n",
      "        u[1:-1,1:-1]=un[1:-1,1:-1]+alpha*dt/dx**2*(un[2:,1:-1]-2*un[1:-1,1:-1]+un[0:-2,1:-1])+alpha*dt/dy**2*(un[1:-1,2:]-2*un[1:-1,1:-1]+un[1:-1,0:-2])\n",
      "        u[0,:]=200\n",
      "        u[-1,:]=0\n",
      "        u[:,0]=200\n",
      "        u[:,-1]=0\n",
      "    \n",
      "    return u\n",
      "\n",
      "fast_diffuse = jit('f8(f8[:,:])')(diffuse_python)\n",
      "flex_diffuse = autojit(diffuse_python)\n",
      "\n",
      "# u_cpu = diffuse_python(64,64)\n",
      "# plot_temp(u_cpu)\n",
      "\n",
      "\n",
      "# pycuda\n",
      "import pycuda.driver as cuda\n",
      "import pycuda.autoinit\n",
      "from pycuda.compiler import SourceModule\n",
      "\n",
      "mod1 = SourceModule(\"\"\"\n",
      "__global__ void copy_array (float *u, float *u_prev, int N, int BSZ)\n",
      "{\t\n",
      "\tint x = blockIdx.x*blockDim.x + threadIdx.x;\n",
      "\tint y = blockIdx.y*blockDim.y + threadIdx.y;\n",
      "\tif ((x<N) && (y<N)) {\n",
      "\t\tint idx = x + y * blockDim.x*gridDim.x;\n",
      "\t\tu_prev[idx] = u[idx];\n",
      "\t}\n",
      "}\n",
      "\"\"\")\n",
      "\n",
      "mod2 = SourceModule(\"\"\"\n",
      "__global__ void update (float *u, float *u_prev, int N, float h, float dt, float alpha, int BSZ)\n",
      "{\n",
      "\tint x = blockIdx.x*blockDim.x + threadIdx.x;\n",
      "\tint y = blockIdx.y*blockDim.y + threadIdx.y;\n",
      "\t\n",
      "\tif (x>0 && (x<N-1) && y>0 && (y<N-1)) {\n",
      "\t\tint idx = x + y * blockDim.x*gridDim.x;\n",
      "\t\tu[idx] = u_prev[idx] + alpha*dt/(h*h) * (u_prev[idx+1] + u_prev[idx-1] + u_prev[idx+N] + u_prev[idx-N] - 4*u_prev[idx]);\n",
      "\t}\n",
      "}\n",
      "\"\"\")\n",
      "\n",
      "copy_array = mod1.get_function(\"copy_array\")\n",
      "update = mod2.get_function('update')\n",
      "\n",
      "def diffuse_pycuda(u):\n",
      "    \n",
      "    nx,ny = np.int32(u.shape)\n",
      "    alpha = np.float32(0.645)\n",
      "    dx = np.float32(3.5/(nx-1))\n",
      "    dy = np.float32(3.5/(ny-1))\n",
      "    dt = np.float32(1e-05)\n",
      "    time = np.float32(0.4)\n",
      "    nt = np.int32(np.ceil(time/dt))\n",
      "#     print nt\n",
      "    \n",
      "    u[0,:]=200\n",
      "    u[:,0]=200  \n",
      "    \n",
      "    u = u.astype(np.float32)\n",
      "    \n",
      "    u_prev = u.copy()    \n",
      "    \n",
      "    u_d = cuda.mem_alloc(u.size*u.dtype.itemsize)\n",
      "    u_prev_d = cuda.mem_alloc(u_prev.size*u_prev.dtype.itemsize)\n",
      "    cuda.memcpy_htod(u_d, u)\n",
      "    cuda.memcpy_htod(u_prev_d, u_prev)\n",
      "\n",
      "    BLOCKSIZE = 16\n",
      "    gridSize = (int(np.ceil(nx/BLOCKSIZE)),int(np.ceil(nx/BLOCKSIZE)),1)\n",
      "    blockSize = (BLOCKSIZE,BLOCKSIZE,1)\n",
      "\n",
      "    for t in range(nt+1):\n",
      "        copy_array(u_d, u_prev_d, nx, np.int32(BLOCKSIZE), block=blockSize, grid=gridSize)\n",
      "        update(u_d, u_prev_d, nx, dx, dt, alpha, np.int32(BLOCKSIZE), block=blockSize, grid=gridSize)\n",
      "    \n",
      "    cuda.memcpy_dtoh(u, u_d)\n",
      "    \n",
      "    return u\n",
      "\n",
      "super_fast_diffuse = jit('f8(f8[:,:])')(diffuse_pycuda)\n",
      "super_flex_diffuse = autojit(diffuse_pycuda)\n",
      "\n",
      "# u_gpu = diffuse_pycuda(64,64)\n",
      "# plot_temp(u_gpu)\n",
      "\n",
      "# plotting\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D \n",
      "from matplotlib import cm\n",
      "\n",
      "def plot_temp(u):\n",
      "    nx,ny = u.shape\n",
      "    x = np.linspace(0,3.5,nx)\n",
      "    y = np.linspace(0,3.5,ny)\n",
      "    X,Y = np.meshgrid(x,y)\n",
      "    #         surf = \n",
      "    plt.pcolormesh(X,Y,u[:], cmap=cm.gnuplot)\n",
      "    plt.axes().set_aspect('equal', 'box')\n",
      "    plt.show()\n",
      "#         plt.savefig('sol_%s.png'%nt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting diffuse.py\n"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}